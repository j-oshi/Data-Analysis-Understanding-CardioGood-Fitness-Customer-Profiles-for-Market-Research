import os
import tempfile
import shutil
import pymupdf
import streamlit as st
import logging
from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain.schema import Document
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from typing import List, Tuple, Dict, Any, Optional
import ollama

# Logging configuration
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

# Streamlit page configuration
st.set_page_config(
    page_title="PDF RAG Streamlit App",
    page_icon="ðŸ“„",
    layout="wide",
    initial_sidebar_state="collapsed",
)

@st.cache_resource(show_spinner=True)
def extract_model_names(
    _models_info: Dict[str, List[Dict[str, Any]]],
) -> Tuple[str, ...]:
    """
    Extract model names from the provided models information.

    Args:
        models_info (Dict[str, List[Dict[str, Any]]]): Dictionary containing information about available models.

    Returns:
        Tuple[str, ...]: A tuple of model names.
    """
    logger.info("Extracting model names from models_info")
    model_names = tuple(model["model"] for model in _models_info["models"])
    logger.info(f"Extracted model names: {model_names}")
    return model_names


@st.cache_data
def load_pdf_with_pymupdf(file_path):
    """
    Load text from a PDF file using PyMuPDF (fitz).
    Returns the extracted text as a string.
    """
    try:
        doc = pymupdf.open(file_path)
        text = ""
        for page in doc:
            text += page.get_text()
        doc.close()
        return text
    except Exception as e:
        logger.error(f"Error loading PDF: {e}")
        return None

@st.cache_resource
def create_vector_db(pdf_text, model="mistral:latest"):
    """
    Create a vector database from the given PDF text.

    Args:
        pdf_text (str): Extracted text from the PDF file.
        model (str): Model to use for embedding.

    Returns:
        Chroma: A vector database containing the processed document chunks.
    """
    documents = [Document(page_content=pdf_text, metadata={})]
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)
    chunks = text_splitter.split_documents(documents)

    vector_db = Chroma.from_documents(
        documents=chunks,
        embedding=OllamaEmbeddings(model=model),
        collection_name="local-rag"
    )
    return vector_db

@st.cache_resource
def create_retriever(_vector_db, _llm):
  """
  Create a retriever using the vector database and language model.

  Args:
      _vector_db (Chroma, optional): Vector database for retrieval. Defaults to None.
      _llm (ChatOllama, optional): Language model for multi-query retrieval. Defaults to None.

  Returns:
      MultiQueryRetriever: Configured retriever instance, or None if inputs are None.
  """
  if _vector_db is not None and _llm is not None:
    QUERY_PROMPT = PromptTemplate(
      input_variables=["question"],
      template="""You are an AI language model assistant. Your task is to generate five question-answering variations of the given user question to retrieve relevant documents from a vector database. By framing the query as potential answers to a question, your goal is to identify documents that directly address the user's information need. Provide these alternative questions separated by newlines.
        Original question: {question}""",
    )

    retriever = MultiQueryRetriever.from_llm(
      _vector_db.as_retriever(),
      _llm,
      prompt=QUERY_PROMPT
    )
    return retriever
  else:
    return None

@st.cache_data
def process_question(question, _vector_db, selected_model):
    """
    Process a question using the retriever and language model.

    Args:
        retriever (MultiQueryRetriever): Configured retriever instance.
        question (str): User question.
        llm (ChatOllama): Language model for answering questions.

    Returns:
        str: Response generated by the language model.
    """

    # if "llm" not in st.session_state or st.session_state["llm"] is None:
    llm = ChatOllama(model=selected_model)
    
    template = """Answer the question based ONLY on the following context:\n    {context}\n    Question: {question}\n    """
    prompt = ChatPromptTemplate.from_template(template)

    QUERY_PROMPT = PromptTemplate(
      input_variables=["question"],
      template="""You are an AI language model assistant. Your task is to generate five question-answering variations of the given user question to retrieve relevant documents from a vector database. By framing the query as potential answers to a question, your goal is to identify documents that directly address the user's information need. Provide these alternative questions separated by newlines.
        Original question: {question}""",
    )

    retriever = MultiQueryRetriever.from_llm(
      _vector_db.as_retriever(),
      llm,
      prompt=QUERY_PROMPT
    )


    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain.invoke(question)

def main():
    # Initialize session state keys
    if "vector_db" not in st.session_state:
        st.session_state["vector_db"] = None
    
    if "llm" not in st.session_state:
        st.session_state["llm"] = None

    st.title("PDF Retrieval Augmented Generation (RAG)")

    # Get list of avialable LLM models
    models_info = ollama.list()
    available_models = extract_model_names(models_info)

    # Create layout
    col1, col2 = st.columns([4, 1])

    # Model selection
    if available_models:
        selected_model = col2.selectbox(
            "Select local model", 
            available_models,
            key="model_select"
        )

    # File uploader for the PDF
    uploaded_file = col1.file_uploader("Upload a PDF file", type="pdf")

    if uploaded_file is not None:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
            temp_file.write(uploaded_file.read())
            temp_path = temp_file.name
        pdf_text = load_pdf_with_pymupdf(temp_path)
 
        if pdf_text:
            col1.write("PDF loaded successfully! Setting up vector database...")
            with st.spinner("Processing document..."):
                st.session_state["vector_db"] = create_vector_db(pdf_text)
            col1.success("Vector database created!")
        else:
            col1.error("Failed to extract text from the PDF.")
    
    # Ensure vector_db exists before proceeding
    if st.session_state["vector_db"]:
        st.write("Vector database is ready!")
        
        # Query input
        question = col1.text_input("Enter your question:")
        if question:
            with st.spinner("Retrieving answer..."):
                response = process_question(question, st.session_state["vector_db"], selected_model)
                st.write("Answer:", response)

if __name__ == "__main__":
    main()
